from __future__ import annotations

import os
import re
import sys
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Sequence

import torch
from tqdm import tqdm
from transformers import PreTrainedTokenizer


@dataclass
class BaseVisualizer(ABC):
    tokenizer: PreTrainedTokenizer

    @abstractmethod
    def visualize(self, history: list[torch.Tensor | list], **kwargs):
        raise NotImplementedError


@dataclass
class VideoVisualizer(BaseVisualizer):

    def visualize(
        self,
        history: list[torch.Tensor | list],
        output_path: str = "visualization.gif",
        **kwargs,
    ):
        raise NotImplementedError


@dataclass
class TerminalVisualizer(BaseVisualizer):

    # Configuration (adjust as needed)
    HEADER_SIZE = 3  # Fixed number of lines for the header (0 if show_header is False)
    PROGRESS_SIZE = 3  # Fixed number of lines for the progress bar
    PANEL_PADDING_TOP = 1  # Top padding of the Panel (padding=(top, side))
    PANEL_PADDING_BOTTOM = 1  # Bottom padding of the Panel
    PANEL_PADDING_SIDE = 1  # Number of characters used for left and right padding
    PANEL_BORDER = 2  # Number of columns taken by the Panel border (usually 2)
    MIN_TOTAL_HEIGHT = 10  # Minimum terminal height (in lines)
    MAX_TOTAL_HEIGHT = 60  # Maximum terminal height to prevent overflowing the terminal
    DEFAULT_TERM_WIDTH = 120  # Default terminal width (in columns)
    ansi_escape = re.compile(r"\x1b\[[0-9;]*m")  # Regex to match ANSI escape codes

    def visualize(
        self,
        history: list[torch.Tensor],  # list of tokens per step: [T] or [B,T]
        fps: int = 16,
        rich: bool = True,
        title: str = "dllm",
        max_chars: int = None,
        every_n_steps: int = 1,
        show_header: bool = True,
        skip_special_tokens: bool = False,
        info_gain_scores: list[torch.Tensor] | None = None,  # Info-Gain scores per step
        candidate_tokens: list[torch.Tensor] | None = None,  # Candidate tokens per step
    ) -> None:
        """
        Visualize a masked-diffusion decoding trajectory stored in `history`.
        If items have batch dimension [B, T], visualize each sequence separately.
        """
        try:
            # detect batch size
            first_step = history[0]
            if first_step.dim() > 1 and first_step.shape[0] > 1:
                B = first_step.shape[0]
                for b_idx in range(B):
                    # build per-sequence history
                    seq_history = [step[b_idx].unsqueeze(0) for step in history]
                    seq_scores = None
                    if info_gain_scores:
                        seq_scores = [score[b_idx].unsqueeze(0) if score is not None and score.dim() > 1 else score for score in info_gain_scores]
                    seq_cand_tokens = None
                    if candidate_tokens:
                        seq_cand_tokens = [tokens[b_idx].unsqueeze(0) if tokens is not None and tokens.dim() > 1 else tokens for tokens in candidate_tokens]
                    self.visualize_one_history(
                        seq_history,
                        fps,
                        rich,
                        title=f"{title} (Batch {b_idx})",
                        max_chars=max_chars,
                        every_n_steps=every_n_steps,
                        show_header=show_header,
                        skip_special_tokens=skip_special_tokens,
                        info_gain_scores=seq_scores,
                        candidate_tokens=seq_cand_tokens,
                    )
            else:
                # no batch, just visualize normally
                self.visualize_one_history(
                    history,
                    fps,
                    rich,
                    title,
                    max_chars,
                    every_n_steps,
                    show_header,
                    skip_special_tokens,
                    info_gain_scores,
                    candidate_tokens,
                )
        except Exception as e:
            print(f"(Visualization skipped due to error: {e})")

    def visualize_one_history(
        self,
        history: list[torch.Tensor],  # list of tokens per step: [T] or [B,T]
        fps: int = 16,
        rich: bool = True,
        title: str = "dllm",
        max_chars: int = None,
        every_n_steps: int = 1,  # re-render frequency (perf knob)
        show_header: bool = True,
        skip_special_tokens: bool = False,  # NEW ARGUMENT
        info_gain_scores: list[torch.Tensor] | None = None,  # Info-Gain scores per step
        candidate_tokens: list[torch.Tensor] | None = None,  # Candidate tokens per step
    ) -> None:
        """
        Visualize a masked-diffusion decoding trajectory stored in `history`.

        Args:
            history: Sequence of token tensors for each step. Each item is [T] or [B,T].
            fps: Frames per second for the live UI (Rich) or sleep cadence for tqdm fallback.
            title: Header title.
            max_chars: Cap on rendered characters to keep terminal snappy.
            every_n_steps: Only redraw text every N steps (progress still updates every step).
            show_header: Show the magenta header bar (Rich path).
            skip_special_tokens: Whether to skip special/pad/eos tokens when rendering (default: False).
        Notes:
            - Masked positions are detected via `self.tokenizer.mask_token_id`.
            - Special tokens are determined via `self.tokenizer.all_special_ids`.
            - All layout, styling, and progress are encapsulated here.
        """
        # --------- imports & env checks ----------
        try:
            from rich.console import Console
            from rich.layout import Layout
            from rich.live import Live
            from rich.panel import Panel
            from rich.progress import (
                BarColumn,
                MofNCompleteColumn,
                Progress,
                SpinnerColumn,
                TextColumn,
                TimeRemainingColumn,
            )
            from rich.text import Text

            _RICH_IMPORTED = True
        except Exception:
            _RICH_IMPORTED = False

        try:
            from tqdm import tqdm

            _TQDM_IMPORTED = True
        except Exception:
            _TQDM_IMPORTED = False

        if self.tokenizer is None:
            raise ValueError(
                "TerminalVisualizer.tokenizer must be set to a valid tokenizer."
            )

        tokenizer = self.tokenizer
        specials: set[int] = set(getattr(tokenizer, "all_special_ids", []) or [])
        self._specials = specials  # store for helpers
        self._mask_token_id: Optional[int] = getattr(tokenizer, "mask_token_id", None)
        self._pad_token_id: Optional[int] = getattr(tokenizer, "pad_token_id", None)
        self._eos_token_id: Optional[int] = getattr(tokenizer, "eos_token_id", None)

        # --------- helpers inside class scope ----------
        # (keep everything inside this class as requested)

        # throttle settings
        sleep_s = 0.0 if fps <= 0 else 1.0 / float(max(1, fps))
        total_steps = len(history)
        every_n_steps = max(1, int(every_n_steps))

        # decode final text up-front (used after render)
        final_text = self._detok(history[-1], skip_special_tokens=skip_special_tokens)
        final_text = self._truncate(final_text, max_chars)

        # ------------------ new: estimate height from final_text ------------------
        import shutil
        import textwrap

        def strip_ansi(s: str) -> str:
            return self.ansi_escape.sub("", s) if s else ""

        def estimate_height_from_text(text: str, console_width: int) -> int:
            """
            Estimate how many terminal rows the panel with `text` will need given console_width.
            Uses class constants for paddings/borders and header/progress sizes.
            """
            plain = strip_ansi(text or "")
            # inner width = console width minus left/right panel paddings & border
            inner_width = max(
                10, console_width - 2 * self.PANEL_PADDING_SIDE - self.PANEL_BORDER
            )
            lines = 0
            # preserve existing newlines: wrap each paragraph separately
            for para in plain.splitlines() or [""]:
                if para.strip() == "":
                    lines += 1
                    continue
                wrapped = textwrap.wrap(
                    para,
                    width=inner_width,
                    replace_whitespace=False,
                    drop_whitespace=False,
                )
                lines += max(1, len(wrapped))
            text_block_lines = (
                lines + self.PANEL_PADDING_TOP + self.PANEL_PADDING_BOTTOM
            )
            extra = 2  # for panel title / subtitle / small margin
            header_h = self.HEADER_SIZE if show_header else 0
            total = header_h + text_block_lines + self.PROGRESS_SIZE + extra
            # clamp
            total = max(self.MIN_TOTAL_HEIGHT, min(total, self.MAX_TOTAL_HEIGHT))
            return int(total)

        # try to detect terminal width; fallback to 100
        try:
            term_width = shutil.get_terminal_size().columns
            if not isinstance(term_width, int) or term_width <= 0:
                term_width = self.DEFAULT_TERM_WIDTH
        except Exception:
            term_width = self.DEFAULT_TERM_WIDTH

        est_height = estimate_height_from_text(final_text, console_width=term_width)
        # ------------------ end new ----------------------------------------------

        # choose rich or tqdm
        use_rich = bool(rich and _RICH_IMPORTED)

        if not use_rich or not _RICH_IMPORTED:
            # ---------- tqdm fallback ----------
            if not _TQDM_IMPORTED:
                for i, toks in enumerate(history, start=1):
                    if sleep_s > 0:
                        time.sleep(sleep_s)
                print("\n✨ Generation complete!\n")
                print(final_text)
                return

            pbar = tqdm(total=total_steps, desc="Diffusion", leave=True)
            for i, toks in enumerate(history, start=1):
                pbar.update(1)
                pbar.set_postfix(
                    {
                        "masks": self._count_masks(toks),
                        "pct": f"{int(100 * i / max(total_steps, 1))}%",
                    }
                )
                if sleep_s > 0:
                    time.sleep(sleep_s)
            pbar.close()
            print("\n✨ Generation complete!\n")
            if final_text:
                print(final_text)
            return

        # ---------- rich live UI ----------
        # replaced fixed height=100 with the estimated height from history[-1]
        console = Console(
            force_terminal=True,
            color_system="truecolor",
            width=term_width,
            height=est_height,
        )
        layout = Layout()
        layout.split_column(
            (
                Layout(name="header", size=3)
                if show_header
                else Layout(name="header", size=0)
            ),
            Layout(name="text", ratio=1),
            Layout(name="progress", size=3),
        )

        progress = Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]Diffusion"),
            BarColumn(),
            MofNCompleteColumn(),
            TextColumn("•"),
            TextColumn("[cyan]Masks: {task.fields[masks]}"),
            TextColumn("•"),
            TextColumn("[magenta]{task.fields[pct]:>4s}"),
            TimeRemainingColumn(),
            expand=True,
        )

        init_masks = self._count_masks(history[0]) if history else 0
        task_id = progress.add_task(
            "Generating", total=total_steps, masks=init_masks, pct="0%"
        )

        with Live(layout, console=console, refresh_per_second=max(1, fps)):
            for step_idx, toks in enumerate(history, start=1):
                if show_header:
                    header = Text(title, style="bold magenta", justify="center")
                    layout["header"].update(Panel(header, border_style="bright_blue"))

                # progress bar
                masks_remaining = self._count_masks(toks)
                pct = f"{int(100 * step_idx / max(total_steps, 1))}%"
                progress.update(task_id, advance=1, masks=masks_remaining, pct=pct)

                # text panel: decode whole sequence (avoids Ġ/Ċ artifacts)
                # Always render to show real-time updates with Info-Gain colors
                # Get info-gain scores and candidate tokens for current step if available
                step_scores = None
                if info_gain_scores and step_idx <= len(info_gain_scores):
                    step_scores = info_gain_scores[step_idx - 1]
                step_cand_tokens = None
                if candidate_tokens and step_idx <= len(candidate_tokens):
                    step_cand_tokens = candidate_tokens[step_idx - 1]
                
                text_rich = self._detok_with_info_gain(
                    toks, step_scores, step_cand_tokens, skip_special_tokens=skip_special_tokens
                )
                if text_rich is None:
                    text_str = self._detok(toks, skip_special_tokens=skip_special_tokens)
                    text_str = self._truncate(text_str, max_chars)
                    text_rich = Text.from_ansi(text_str) if text_str else Text("")
                else:
                    # Truncate if needed (approximate)
                    if max_chars:
                        # Simple truncation - could be improved
                        pass
                
                layout["text"].update(
                    Panel(
                        (
                            text_rich
                            if (hasattr(text_rich, 'plain') and text_rich.plain) or (isinstance(text_rich, str) and text_rich)
                            else Text("[dim]— no tokens —[/dim]")
                        ),
                        title="[bold]Generated Text (Info-Gain Candidates Colored)",
                        subtitle=f"[dim]Step {step_idx}/{total_steps} - Red=Low Score, Blue=High Score[/dim]",
                        border_style="cyan",
                        padding=(1, 1),
                    )
                )

                layout["progress"].update(Panel(progress))
                if sleep_s > 0:
                    time.sleep(sleep_s)

        console.print("\n[bold green]✨ Generation complete![/bold green]\n")
        # console.print(
        #     Panel(
        #         final_text if final_text else "[dim]— no decodable text —[/dim]",
        #         title="[bold]Final Generated Text",
        #         border_style="green",
        #         padding=(1, 2),
        #     )
        # )

    # ======================== helpers (kept inside class) ========================

    def _has_tty(self) -> bool:
        return sys.stdout.isatty() and os.environ.get("TERM", "") not in ("", "dumb")

    def _first_item(self, x: torch.Tensor) -> torch.Tensor:
        return x[0] if x.dim() > 1 else x

    def _count_masks(self, toks: torch.Tensor) -> int:
        if getattr(self, "_mask_token_id", None) is None:
            return 0
        t = self._first_item(toks)
        return int((t == self._mask_token_id).sum().item())

    def _detok(self, ids_or_tensor, *, skip_special_tokens: bool) -> str:
        """
        Robust detokenize for list[int] / torch.Tensor([T]) / torch.Tensor([B,T]).
        Decode the whole sequence to avoid byte-level artifacts like Ġ/Ċ.
        """
        tokenizer = self.tokenizer
        # normalize to python list[int]
        if isinstance(ids_or_tensor, torch.Tensor):
            t = self._first_item(ids_or_tensor).long()
            ids = t.tolist()
        elif isinstance(ids_or_tensor, (list, tuple)):
            ids = list(ids_or_tensor)
        else:
            # unknown type
            return ""

        # Optionally drop specials/pad/eos *before* decode if desired
        if skip_special_tokens:
            keep = []
            specials = getattr(self, "_specials", set())
            pad_id = getattr(self, "_pad_token_id", None)
            eos_id = getattr(self, "_eos_token_id", None)
            for tid in ids:
                if tid in specials:
                    continue
                if pad_id is not None and tid == pad_id:
                    continue
                if eos_id is not None and tid == eos_id:
                    continue
                keep.append(tid)
            ids = keep

        # Prefer tokenizer.decode (handles Ġ/Ċ, merges properly)
        text = ""
        try:
            if hasattr(tokenizer, "decode"):
                text = tokenizer.decode(
                    ids,
                    skip_special_tokens=False,
                    clean_up_tokenization_spaces=True,
                )
            else:
                # fallback: tokens -> string
                toks = tokenizer.convert_ids_to_tokens(ids)
                if hasattr(tokenizer, "convert_tokens_to_string"):
                    text = tokenizer.convert_tokens_to_string(toks)
                else:
                    text = " ".join(map(str, toks))
        except Exception:
            # extremely defensive fallback
            try:
                text = tokenizer.decode(ids, skip_special_tokens=True)
            except Exception:
                text = ""

        # sanitize control chars for terminal
        if text:
            text = text.replace("\r", "")
        return text

    def _truncate(self, s: str, max_chars: Optional[int]) -> str:
        if max_chars is None or (isinstance(max_chars, int) and max_chars < 0):
            return s
        return s[:max_chars]

    def _score_to_color(self, score: float, min_score: float, max_score: float) -> str:
        """
        Convert info-gain score to color (red=low, blue=high).
        Returns Rich color style string.
        """
        if min_score == max_score or not (min_score <= score <= max_score):
            return "white"
        
        # Normalize to [0, 1]
        normalized = (score - min_score) / (max_score - min_score)
        
        # Map to RGB: red (1,0,0) -> blue (0,0,1)
        # Red component decreases, Blue component increases
        red = int(255 * (1 - normalized))
        blue = int(255 * normalized)
        green = 0
        
        return f"rgb({red},{green},{blue})"

    def _detok_with_info_gain(
        self, ids_or_tensor, scores: torch.Tensor | None, candidate_tokens: torch.Tensor | None = None, *, skip_special_tokens: bool
    ) -> Text | None:
        """
        Detokenize with Info-Gain score coloring.
        Returns Rich Text object with colored tokens, or None if scores not available.
        """
        if scores is None:
            return None
        
        try:
            from rich.text import Text
        except ImportError:
            return None
        
        tokenizer = self.tokenizer
        # normalize to python list[int]
        if isinstance(ids_or_tensor, torch.Tensor):
            t = self._first_item(ids_or_tensor).long()
            ids = t.tolist()
        elif isinstance(ids_or_tensor, (list, tuple)):
            ids = list(ids_or_tensor)
        else:
            return None
        
        # Get scores as list
        if isinstance(scores, torch.Tensor):
            scores_t = self._first_item(scores)
            scores_list = scores_t.tolist()
        else:
            return None
        
        # Filter out invalid scores and get valid range
        valid_scores = [
            s for s in scores_list 
            if s != float('-inf') and s != float('inf') and not (isinstance(s, float) and (s != s))  # Exclude NaN
        ]
        if not valid_scores:
            return None
        
        min_score = min(valid_scores)
        max_score = max(valid_scores)
        
        # If all scores are the same, use a small range to enable coloring
        if min_score == max_score:
            max_score = min_score + 1e-6
        
        # Get candidate tokens as list
        cand_tokens_list = None
        if candidate_tokens is not None:
            if isinstance(candidate_tokens, torch.Tensor):
                cand_tokens_t = self._first_item(candidate_tokens)
                cand_tokens_list = cand_tokens_t.tolist()
        
        # Decode tokens one by one to apply colors
        eos_id = getattr(self, "_eos_token_id", None)
        mask_id = getattr(self, "_mask_token_id", None)
        result = Text()
        
        # Build a mapping of token positions to scores
        # Note: scores_list should align with token positions
        for i, tid in enumerate(ids):
            # Skip special tokens if requested
            if skip_special_tokens:
                specials = getattr(self, "_specials", set())
                pad_id = getattr(self, "_pad_token_id", None)
                if tid in specials or (pad_id is not None and tid == pad_id) or (eos_id is not None and tid == eos_id):
                    continue
            
            # Check if this is a mask position with candidate token
            use_candidate = False
            cand_tid = tid
            if cand_tokens_list and i < len(cand_tokens_list):
                cand_tid_from_list = cand_tokens_list[i]
                # If candidate token is not mask_id, use it for display
                if cand_tid_from_list != mask_id and cand_tid_from_list != tid:
                    use_candidate = True
                    cand_tid = cand_tid_from_list
            
            # Get token text (use candidate token if available)
            display_tid = cand_tid if use_candidate else tid
            try:
                token_text = tokenizer.decode([display_tid], skip_special_tokens=False)
            except Exception:
                token_text = tokenizer.convert_ids_to_tokens([display_tid])[0] if hasattr(tokenizer, "convert_ids_to_tokens") else str(display_tid)
            
            # Apply color based on score for all tokens with valid scores
            # Color ALL candidate positions considered by Info-Gain (not just selected ones)
            if i < len(scores_list):
                score = scores_list[i]
                # Check for valid score (not -inf, inf, or NaN)
                is_valid = (
                    score != float('-inf') and 
                    score != float('inf') and 
                    not (isinstance(score, float) and (score != score))  # Check for NaN
                )
                if is_valid:
                    # Color all tokens with valid scores (these are candidate positions considered by Info-Gain)
                    color = self._score_to_color(score, min_score, max_score)
                    result.append(token_text, style=color)
                else:
                    # No score for this position, render normally
                    result.append(token_text)
            else:
                # No score data for this position
                result.append(token_text)
        
        return result if result.plain else None


if __name__ == "__main__":
    pass
